{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Seed set to 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lightning import pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from chemprop import data, featurizers, models, nn\n",
    "pl.seed_everything(1)\n",
    "from utils import *\n",
    "# from data_utils import data_prep, prep_data, load_data, split_data, preprocess_data, create_data_loaders, create_mpnn_model\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\chemprop\\notebooks\\big_data\\data\\ir_dataset_minmax_scaled.parquet' # path to your data .csv file\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = \"smiles\" # name of the column containing SMILES strings\n",
    "target_columns = np.arange(400,4002,2).astype(str) # list of names of the columns containing targets\n",
    "\n",
    "df_input = pd.read_parquet(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_test_path = r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\data\\nist\\nist_test.parquet\"\n",
    "\n",
    "nist_test_df = pd.read_parquet(nist_test_path)\n",
    "\n",
    "test_datapoints = get_mol_datapoints(nist_test_df, smiles_column, target_columns)\n",
    "\n",
    "test_dset = data.MoleculeDataset(test_datapoints, featurizers.SimpleMoleculeMolGraphFeaturizer())\n",
    "\n",
    "test_loader = data.MolGraphDataLoader(test_dset, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m     test_dset \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mMoleculeDataset(test_data, featurizer)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mMolGraphDataLoader(test_dset, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m rndm_test_loader \u001b[38;5;241m=\u001b[39m create_rndm_loader(\u001b[43mtest_1\u001b[49m[:\u001b[38;5;241m700\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_1' is not defined"
     ]
    }
   ],
   "source": [
    "def create_rndm_loader(test_data):\n",
    "    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "\n",
    "    test_dset = data.MoleculeDataset(test_data, featurizer)\n",
    "\n",
    "    return data.MolGraphDataLoader(test_dset, num_workers=0)\n",
    "\n",
    "rndm_test_loader = create_rndm_loader(test_1[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = get_mol_datapoints(df_input, smiles_column, target_columns)\n",
    "\n",
    "train_idx, val_idx, test_idx = data.splitting.split_data(\n",
    "    datapoints,\n",
    "    num_folds=8,\n",
    "    split='cv',\n",
    ")\n",
    "\n",
    "train_1 = [datapoints[i] for i in train_idx[0]]\n",
    "val_1 = [datapoints[i] for i in val_idx[0]]\n",
    "test_1 = [datapoints[i] for i in test_idx[0]]\n",
    "\n",
    "train_2 = [datapoints[i] for i in train_idx[1]]\n",
    "val_2 = [datapoints[i] for i in val_idx[1]]\n",
    "test_2 = [datapoints[i] for i in test_idx[1]]\n",
    "\n",
    "train_3 = [datapoints[i] for i in train_idx[2]]\n",
    "val_3 = [datapoints[i] for i in val_idx[2]]\n",
    "test_3 = [datapoints[i] for i in test_idx[2]]\n",
    "\n",
    "train_4 = [datapoints[i] for i in train_idx[3]]\n",
    "val_4 = [datapoints[i] for i in val_idx[3]]\n",
    "test_4 = [datapoints[i] for i in test_idx[3]]\n",
    "\n",
    "train_5 = [datapoints[i] for i in train_idx[4]]\n",
    "val_5 = [datapoints[i] for i in val_idx[4]]\n",
    "test_5 = [datapoints[i] for i in test_idx[4]]\n",
    "\n",
    "train_6 = [datapoints[i] for i in train_idx[5]]\n",
    "val_6 = [datapoints[i] for i in val_idx[5]]\n",
    "test_6= [datapoints[i] for i in test_idx[5]]\n",
    "\n",
    "train_7 = [datapoints[i] for i in train_idx[6]]\n",
    "val_7 = [datapoints[i] for i in val_idx[6]]\n",
    "test_7 = [datapoints[i] for i in test_idx[6]]\n",
    "\n",
    "train_8 = [datapoints[i] for i in train_idx[7]]\n",
    "val_8 = [datapoints[i] for i in val_idx[7]]\n",
    "test_8 = [datapoints[i] for i in test_idx[7]]\n",
    "\n",
    "def create_data_loaders(train_data, val_data, test_data, num_workers=0):\n",
    "    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "    train_dset = data.MoleculeDataset(train_data, featurizer)\n",
    "    val_dset = data.MoleculeDataset(val_data, featurizer)\n",
    "    test_dset = data.MoleculeDataset(test_data, featurizer)\n",
    "\n",
    "    train_loader = data.MolGraphDataLoader(train_dset, num_workers=num_workers)\n",
    "    val_loader = data.MolGraphDataLoader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "    test_loader = data.MolGraphDataLoader(test_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader_1, val_loader_1, test_loader_1 = create_data_loaders(train_1, val_1, test_1, num_workers)\n",
    "train_loader_2, val_loader_2, test_loader_2 = create_data_loaders(train_2, val_2, test_2, num_workers)\n",
    "train_loader_3, val_loader_3, test_loader_3 = create_data_loaders(train_3, val_3, test_3, num_workers)\n",
    "train_loader_4, val_loader_4, test_loader_4 = create_data_loaders(train_4, val_4, test_4, num_workers)\n",
    "train_loader_5, val_loader_5, test_loader_5 = create_data_loaders(train_5, val_5, test_5, num_workers)\n",
    "train_loader_6, val_loader_6, test_loader_6 = create_data_loaders(train_6, val_6, test_6, num_workers)\n",
    "train_loader_7, val_loader_7, test_loader_7 = create_data_loaders(train_7, val_7, test_7, num_workers)\n",
    "train_loader_8, val_loader_8, test_loader_8 = create_data_loaders(train_8, val_8, test_8, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(metric: str, agg: str = 'mean'):\n",
    "    mp = nn.BondMessagePassing(\n",
    "    depth = 6,\n",
    "    d_h = 2200,\n",
    "    dropout=0.05,\n",
    "    # activation='SILU'\n",
    ")\n",
    "    if agg == 'mean':\n",
    "        agg = nn.MeanAggregation(\n",
    "\n",
    "        )\n",
    "\n",
    "    elif agg == 'attentive':\n",
    "        agg = nn.AttentiveAggregation(\n",
    "            output_size=2200,\n",
    "        )\n",
    "\n",
    "    elif agg == 'sum': \n",
    "        agg = nn.SumAggregation(\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Invalid aggregation: {agg}')\n",
    "\n",
    "    # agg = nn.AttentiveAggregation(\n",
    "    #     output_size=2200,\n",
    "    # )\n",
    "\n",
    "    ffn = nn.RegressionFFN(\n",
    "        input_dim=2200,\n",
    "        n_layers=6,\n",
    "        hidden_dim=2200,\n",
    "        dropout=0.05,\n",
    "        # activation='SILU',\n",
    "        # loc=scaler.mean_, # pass in the mean of the training targets\n",
    "        # scale=scaler.scale_,\n",
    "        n_tasks=1801 # pass in the scale of the training targets\n",
    "    )\n",
    "    batch_norm=False\n",
    "    if metric == 'rmse':\n",
    "        metric_list = [nn.RMSEMetric()]\n",
    "    elif metric == 'sid':\n",
    "        metric_list = [nn.SIDMetric(), nn.RMSEMetric()]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Invalid metric: {metric}')\n",
    "    return models.MPNN(mp, agg, ffn, batch_norm, metrics=metric_list)\n",
    "\n",
    "mpnn1 = create_model('rmse', 'attentive')\n",
    "mpnn2 = create_model('rmse', 'attentive')\n",
    "mpnn3 = create_model('rmse', 'attentive')\n",
    "mpnn4 = create_model('rmse', 'attentive')\n",
    "mpnn5 = create_model('rmse', 'attentive')\n",
    "mpnn6 = create_model('rmse', 'attentive')\n",
    "mpnn7 = create_model('rmse', 'attentive')\n",
    "mpnn8 = create_model('rmse', 'attentive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\rhys-\\miniconda3\\envs\\cp3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer_1 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    "    # callbacks=[metric_tracker, early_stop_callback]\n",
    ")\n",
    "\n",
    "trainer_2 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    ")\n",
    "\n",
    "trainer_3 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    "    # callbacks=[metric_tracker, early_stop_callback]\n",
    ")\n",
    "\n",
    "trainer_4 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    ")\n",
    "\n",
    "trainer_5 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    "    # callbacks=[metric_tracker, early_stop_callback]\n",
    ")\n",
    "\n",
    "trainer_6 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    ")\n",
    "\n",
    "trainer_7 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    ")\n",
    "\n",
    "trainer_8 = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=12, # number of epochs to train for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\rhys-\\miniconda3\\envs\\cp3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing   | 10.3 M\n",
      "1 | agg             | AttentiveAggregation | 2.2 K \n",
      "2 | bn              | Identity             | 0     \n",
      "3 | predictor       | RegressionFFN        | 33.0 M\n",
      "  | other params    | n/a                  | 1.8 K \n",
      "---------------------------------------------------------\n",
      "43.3 M    Trainable params\n",
      "1.8 K     Non-trainable params\n",
      "43.3 M    Total params\n",
      "173.278   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rhys-\\miniconda3\\envs\\cp3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  26%|██▌       | 500/1948 [02:30<07:16,  3.32it/s, v_num=100, train/loss=0.0057]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rhys-\\miniconda3\\envs\\cp3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing   | 10.3 M\n",
      "1 | agg             | AttentiveAggregation | 2.2 K \n",
      "2 | bn              | Identity             | 0     \n",
      "3 | predictor       | RegressionFFN        | 33.0 M\n",
      "  | other params    | n/a                  | 1.8 K \n",
      "---------------------------------------------------------\n",
      "43.3 M    Trainable params\n",
      "1.8 K     Non-trainable params\n",
      "43.3 M    Total params\n",
      "173.278   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  29%|██▉       | 565/1948 [00:33<01:21, 16.99it/s, v_num=101, train/loss=0.00521]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing   | 10.3 M\n",
      "1 | agg             | AttentiveAggregation | 2.2 K \n",
      "2 | bn              | Identity             | 0     \n",
      "3 | predictor       | RegressionFFN        | 33.0 M\n",
      "  | other params    | n/a                  | 1.8 K \n",
      "---------------------------------------------------------\n",
      "43.3 M    Trainable params\n",
      "1.8 K     Non-trainable params\n",
      "43.3 M    Total params\n",
      "173.278   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 83/1948 [00:11<04:20,  7.16it/s, v_num=102, train/loss=0.00656]"
     ]
    }
   ],
   "source": [
    "trainer_1.fit(mpnn1, train_loader_1, val_loader_1)\n",
    "trainer_2.fit(mpnn2, train_loader_2, val_loader_2)\n",
    "trainer_3.fit(mpnn3, train_loader_3, val_loader_3)\n",
    "trainer_4.fit(mpnn4, train_loader_4, val_loader_4)\n",
    "trainer_5.fit(mpnn5, train_loader_5, val_loader_5)\n",
    "trainer_6.fit(mpnn6, train_loader_6, val_loader_6)\n",
    "trainer_7.fit(mpnn7, train_loader_7, val_loader_7)\n",
    "trainer_8.fit(mpnn8, train_loader_8, val_loader_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop import models \n",
    "\n",
    "trainer_1.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn1.ckpt')\n",
    "trainer_2.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn2.ckpt')\n",
    "trainer_3.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn3.ckpt')\n",
    "trainer_4.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn4.ckpt')\n",
    "trainer_5.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn5.ckpt')\n",
    "trainer_6.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn6.ckpt')\n",
    "trainer_7.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn7.ckpt')\n",
    "trainer_8.save_checkpoint(r'C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn8.ckpt')\n",
    "\n",
    "checkpoint = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn1.ckpt\")\n",
    "checkpoint2 = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn2.ckpt\")\n",
    "checkpoint3 = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn3.ckpt\")\n",
    "checkpoint4 = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn4.ckpt\")\n",
    "checkpoint5 = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn5.ckpt\")\n",
    "checkpoint6 = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn6.ckpt\")\n",
    "checkpoint7 = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn7.ckpt\")\n",
    "checkpoint8 = torch.load(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\mpnn8.ckpt\")\n",
    "\n",
    "# Assuming 'output_size' needs to be added or corrected\n",
    "# You need to know the correct value for 'output_size'\n",
    "checkpoint['hyper_parameters']['agg']['output_size'] = 2200\n",
    "checkpoint2['hyper_parameters']['agg']['output_size'] = 2200\n",
    "checkpoint3['hyper_parameters']['agg']['output_size'] = 2200\n",
    "checkpoint4['hyper_parameters']['agg']['output_size'] = 2200\n",
    "checkpoint5['hyper_parameters']['agg']['output_size'] = 2200\n",
    "checkpoint6['hyper_parameters']['agg']['output_size'] = 2200\n",
    "checkpoint7['hyper_parameters']['agg']['output_size'] = 2200\n",
    "checkpoint8['hyper_parameters']['agg']['output_size'] = 2200\n",
    "\n",
    "torch.save(checkpoint, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_1.ckpt\")\n",
    "torch.save(checkpoint2, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_2.ckpt\")\n",
    "torch.save(checkpoint3, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_3.ckpt\")\n",
    "torch.save(checkpoint4, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_4.ckpt\")\n",
    "torch.save(checkpoint5, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_5.ckpt\")\n",
    "torch.save(checkpoint6, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_6.ckpt\")\n",
    "torch.save(checkpoint7, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_7.ckpt\")\n",
    "torch.save(checkpoint8, r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_8.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = models.MPNN.load_from_checkpoint(r\"C:\\Users\\rhys-\\OneDrive\\Documents\\Github\\thesis_results\\models\\modified_mpnn_1.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\rhys-\\miniconda3\\envs\\cp3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 325/325 [00:07<00:00, 40.90it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/rmse          0.057879265397787094\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test/rmse': 0.057879265397787094}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_loaded = pl.Trainer(\n",
    "    precision='bf16-mixed',\n",
    "    logger=True,\n",
    "    enable_checkpointing=False, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=6, # number of epochs to train for\n",
    "    # callbacks=[metric_tracker, early_stop_callback]\n",
    ")\n",
    "\n",
    "trainer_loaded.test(loaded_model, test_loader_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
